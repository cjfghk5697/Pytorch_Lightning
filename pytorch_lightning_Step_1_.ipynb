{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch lightning Step.1 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNymYcMNks4Klb7zfqJFVJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/Pytorch_Lightning/blob/main/pytorch_lightning_Step_1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOW TO ORGANIZE PYTORCH INTO LIGHTNING\n"
      ],
      "metadata": {
        "id": "othWj4n2y1Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "PmSuDF1tyFDK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Move your Computational Code\n",
        "Move the model architecture and forward pass to your LightningModule."
      ],
      "metadata": {
        "id": "fu5L94Ysy8sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1=nn.Linear(28*28,128)\n",
        "    self.layer_2=nn.Linear(128,10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x=x.view(x.size(0),-1)\n",
        "    x=self.layer_1(x)\n",
        "    x=F.relu(x)\n",
        "    x=self.layer_2(x)\n",
        "    return x\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n",
        "    return [optimizer], [lr_scheduler]\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    y_hat = self(x)\n",
        "    loss = F.cross_entropy(y_hat, y)\n",
        "    return loss\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      y_hat = self(x)\n",
        "      val_loss = F.cross_entropy(y_hat, y)\n",
        "      self.log(\"val_loss\", val_loss)\n",
        "  def test_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      y_hat = self(x)\n",
        "      test_loss = F.cross_entropy(y_hat, y)\n",
        "      self.log(\"test_loss\", test_loss)\n",
        "  def predict_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      pred = self(x)\n",
        "      return pred"
      ],
      "metadata": {
        "id": "fvLCUkFWyeur"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2. Move the Optimizer(s) and LR Scheduler(s)\n",
        "Move your optimizers to the configure_optimizers() hook.\n",
        "\n",
        "# 3. Configure the Training Logic\n",
        "Lightning help training loop, manges all of the associated comonetns for you. For example, Using training_step() method need batch and batch index. Optionally, it can take optimizer_idx if your LightningModule defines multiple optimizers within its configure_optimizers() hook.\n",
        "\n",
        "# 4. Configure the Validation Logic\n",
        "You see Lightning help Training Logic for you. As you might expect Validation Logic also help you. When using Lightning, simply override the validation_step() method which takes the current batch and the batch_idx as arguments. Optionally, it can take dataloader_idx if you configure multiple dataloaders. To add an (optional) validation loop add logic to the validation_step() hook (make sure to use the hook parameters, batch and batch_idx in this case).\n",
        "\n",
        "And you can run only the validation loop using validate() method.\n",
        "\n"
      ],
      "metadata": {
        "id": "8n6iL0mBzZhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LitModel()\n",
        "trainer.validate(model)"
      ],
      "metadata": {
        "id": "Ohdu7FBjyeGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Congfigure Testing Logic\n",
        "Lightning also help the testing llop for you. (test_step()method.) "
      ],
      "metadata": {
        "id": "q1nEOxuy2mf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LitModel()\n",
        "trainer.test(model)"
      ],
      "metadata": {
        "id": "zFaoKbeE2Q81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Configure Prediction Logic\n",
        "Lightning help prediction Logic.(predict_step())."
      ],
      "metadata": {
        "id": "G-d3YiNc3LM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Coda or to(Device) Calls\n",
        "If you have any explicit calls to .cuda() or .to(device), you can remove them since Lightning makes sure that the data coming from DataLoader and all the Module instances initialized inside LightningModule.__init__ are moved to the respective devices automatically. If you still need to access the current device, you can use self.device anywhere in your LightningModule except in the __init__ and setup methods."
      ],
      "metadata": {
        "id": "cpaIcbSp38D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(LightningModule):\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        z = torch.randn(4, 5, device=self.device)\n",
        "        ..."
      ],
      "metadata": {
        "id": "l25zYSiI4WUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: If you are initializing a Tensor within the LightningModule.__init__ method and want it to be moved to the device automatically you should call register_buffer() to register it as a parameter."
      ],
      "metadata": {
        "id": "Fsryfs1Z4XnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LitModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(num_features))"
      ],
      "metadata": {
        "id": "p-CNAe3k4XXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}